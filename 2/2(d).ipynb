{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYOU3-DKk8H6"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GridEnv:\n",
        "  def __init__(self, grid, gamma, noise):\n",
        "    self.grid = grid\n",
        "    self.gamma = gamma\n",
        "    self.noise = noise\n",
        "\n",
        "    self.size = len(grid)\n",
        "\n",
        "    self.states = [(i, j) for i in range (self.size) for j in range (self.size)]\n",
        "    self.actions= [0, 1, 2, 3]\n",
        "\n",
        "    self.P = {}\n",
        "\n",
        "    for s in self.states:\n",
        "      self.P[s] = {}\n",
        "      if self.isValid(s) == False:\n",
        "        continue\n",
        "\n",
        "      for a in self.actions:\n",
        "        self.P[s][a] = []\n",
        "        next_state, reward = self.take_action(s, a)\n",
        "\n",
        "        exit = (reward == 10 or reward == 0 or reward == -10 or self.isValid(next_state)== False)\n",
        "\n",
        "        for i in range (4):\n",
        "          if i == a:\n",
        "            self.P[s][a].append((4/5, next_state, reward, exit))\n",
        "          else:\n",
        "            next_state, reward = self.take_action(i,s)\n",
        "            self.P[s][a].append((1/15,next_state, reward, exit))\n",
        "\n",
        "\n",
        "  def isValid(self, state):\n",
        "    i, j = state\n",
        "    return 0 <= i < self.size and 0 <= j < self.size and self.grid[i][j] != 'wall'\n",
        "\n",
        "  def take_action(self, a, s):\n",
        "    i, j = s\n",
        "\n",
        "    if a == 0:\n",
        "      j -=1\n",
        "    elif a == 1:\n",
        "      j +=1\n",
        "    elif a == 2:\n",
        "      i -=1\n",
        "    elif a == 3:\n",
        "      i +=1\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action\")\n",
        "\n",
        "    next_state = (i, j)\n",
        "\n",
        "    if self.isValid(next_state):\n",
        "      x = next_state[0]\n",
        "      y = next_state[1]\n",
        "\n",
        "      value = self.grid[x][y]\n",
        "      if (value == 'start' or value == 'wall'):\n",
        "        reward = 0\n",
        "      else:\n",
        "        reward = self.grid[x][y]\n",
        "    else:\n",
        "      reward = self.grid[x][y]\n",
        "\n",
        "    return next_state, reward"
      ],
      "metadata": {
        "id": "LEalZOiFk9KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(env, policy, gamma=0.9, theta=1e-6):\n",
        "  num_states = len(env.states)\n",
        "  n = env.size\n",
        "  V = np.zeros((n, n))\n",
        "\n",
        "  num_iterations = 0\n",
        "  while True:\n",
        "      delta = 0\n",
        "      for i in range(n):\n",
        "        for j in range(n):\n",
        "          s = (i, j)\n",
        "          if not env.isValid(s):\n",
        "            continue\n",
        "          v = V[i][j]\n",
        "          a = policy[s]\n",
        "          for prob, next_state, reward, _ in env.P[s][a]:\n",
        "              v += prob * (reward + gamma * V[next_state[0]][next_state[0]])\n",
        "          V[i][j] = v\n",
        "          delta = max(delta, np.abs(v - V[i][j]))\n",
        "      num_iterations += 1\n",
        "      if delta < theta:\n",
        "          break\n",
        "  return V, num_iterations"
      ],
      "metadata": {
        "id": "IGtBz9xMnQzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_policy(V):\n",
        "  r, c = V.shape\n",
        "  directions = np.zeros((r, c), dtype=int)\n",
        "\n",
        "  for i in range(r):\n",
        "    for j in range(c):\n",
        "      value = V[i][j]\n",
        "      max = value\n",
        "      direction = -1\n",
        "\n",
        "      neighbors = [(i - 1, j), (i + 1, j), (i, j - 1), (i, j + 1)]\n",
        "\n",
        "      for a, b in neighbors:\n",
        "        if 0 <= a < r and 0 <= b < c and V[a, b] > max:\n",
        "          max = V[a, b]\n",
        "          direction = neighbors.index((a, b))\n",
        "\n",
        "        directions[i, j] = direction\n",
        "\n",
        "  return directions"
      ],
      "metadata": {
        "id": "nalb7W1xnTCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, gamma=0.9, theta=1e-6):\n",
        "\n",
        "  num_states = len(env.states)\n",
        "  num_actions = len(env.actions)\n",
        "  n = env.size\n",
        "  V = np.zeros(n, n)\n",
        "\n",
        "  num_iterations = 0\n",
        "  while True:\n",
        "      delta = 0\n",
        "      for i in range(n):\n",
        "        for j in range(n):\n",
        "          s = (i, j)\n",
        "          if not env.isValid(s):\n",
        "            continue\n",
        "          v = V[i][j]\n",
        "          q_values = np.zeros(num_actions)\n",
        "          for a in env.actions:\n",
        "              for prob, next_state, reward, _ in env.P[s][a]:\n",
        "                  q_values[a] += prob * (reward + gamma * V[next_state[0]][next_state[1]])\n",
        "          V[i][j] = max(q_values)\n",
        "          delta = max(delta, abs(v - V[i][j]))\n",
        "      num_iterations += 1\n",
        "      if delta < theta:\n",
        "          break\n",
        "      optimal_policy = find_policy(V)\n",
        "  return optimal_policy, num_iterations, V"
      ],
      "metadata": {
        "id": "D26jQkKTnX8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid = [[0, 0, 0, 0, 0],\n",
        "        [0, 'wall', 0, 0, 0],\n",
        "        [0, 'wall', 1, 'wall', 10],\n",
        "        ['start', 0, 0, 0, 0],\n",
        "        [-10, -10, -10, -10, -10]]\n",
        "\n",
        "gammas = [0.2, 0.5, 0.9]\n",
        "noises = [0.2, 0.3, 0.5]\n",
        "\n",
        "for gamma in gammas:\n",
        "  for noise in noises:\n",
        "    env = GridEnv(grid, gamma, noise)\n",
        "    optimal_policy, num_iterations, V = value_iteration(env, gamma=gamma, theta=1e-6)\n",
        "    print(num_iterations)\n",
        "    print(f\"For Gamma {gamma}, noise {noise} value function is {V}\")\n",
        "\n",
        "    n = len(grid)\n",
        "    for i in range(n):\n",
        "      for j in range(n):\n",
        "        s = (i, j)\n",
        "        if s in env.states:\n",
        "          a = optimal_policy[s[0]][s[1]]\n",
        "          if a == 0:\n",
        "            print(\"left\")\n",
        "          elif a == 1:\n",
        "            print(\"right\")\n",
        "          elif a == 2:\n",
        "            print(\"Up\")\n",
        "          elif a == 3:\n",
        "            print(\"Bottom\")\n",
        "        else:\n",
        "          print(grid[i][j])"
      ],
      "metadata": {
        "id": "FHqVlkVzn-uA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}